**************** MODEL CONFIGURATION ****************
batch_size               -->   12
cased                    -->   True
concat_rnn_layers        -->   True
cuda                     -->   True
cuda_id                  -->   -1
debug                    -->   False
devset                   -->   data/coqa.dev.pipeline.json
dir                      -->   pipeline_models
doc_self_attn            -->   False
dropout_emb              -->   0.5
dropout_ff               -->   0.5
dropout_rnn              -->   0.3
dropout_rnn_output       -->   True
embed_file               -->   wordvecs/glove.840B.300d.txt
embed_size               -->   None
embed_type               -->   glove
f_ner                    -->   False
f_pos                    -->   False
f_qem                    -->   True
fix_embeddings           -->   False
grad_clipping            -->   10.0
hidden_size              -->   300
learning_rate            -->   0.1
max_answer_len           -->   15
max_epochs               -->   5
min_freq                 -->   20
momentum                 -->   0.0
n_history                -->   2
num_layers               -->   3
optimizer                -->   adamax
out_predictions          -->   True
predict_raw_text         -->   False
predict_train            -->   True
pretrained               -->   None
question_merge           -->   self_attn
random_seed              -->   123
resize_rnn_input         -->   False
rnn_padding              -->   False
rnn_type                 -->   lstm
save_params              -->   True
shuffle                  -->   True
span_dependency          -->   True
sum_loss                 -->   False
testset                  -->   None
top_vocab                -->   100000
trainset                 -->   data/coqa.train.pipeline.json
use_qemb                 -->   True
variational_dropout      -->   True
verbose                  -->   400
weight_decay             -->   0.0
word_dropout             -->   False
**************** MODEL CONFIGURATION ****************
<> <> <> Starting Timer [Load data/coqa.train.pipeline.json] <> <> <>
Load 7199 paragraphs, 108647 examples.
Paragraph length: avg = 321.4, max = 1208
Question length: avg = 27.9, max = 442
<> <> <> Finished Timer [Load data/coqa.train.pipeline.json] <> <> <> Total time elapsed: 0h 00m 14s <> <> <>
<> <> <> Starting Timer [Load data/coqa.dev.pipeline.json] <> <> <>
Load 500 paragraphs, 7983 examples.
Paragraph length: avg = 313.8, max = 1003
Question length: avg = 27.8, max = 73
<> <> <> Finished Timer [Load data/coqa.dev.pipeline.json] <> <> <> Total time elapsed: 0h 00m 01s <> <> <>
Train vocab: 83288
Pruned train vocab: 57981
<> <> <> Starting Timer [Load wordvecs/glove.840B.300d.txt] <> <> <>
Embeddings: vocab = 2196018, embed_size = 300
<> <> <> Finished Timer [Load wordvecs/glove.840B.300d.txt] <> <> <> Total time elapsed: 0h 02m 14s <> <> <>
Added word: <Q> (train_freq = 108647)
Added word: <Q1> (train_freq = 101448)
Added word: <A1> (train_freq = 101448)
Added word: <Q2> (train_freq = 94368)
Added word: <A2> (train_freq = 94368)
Added word: Tsarnaev (train_freq = 707)
Added word: Ouattara (train_freq = 692)
Added word: duchy (train_freq = 639)
Added word: Frankish (train_freq = 630)
Added word: XXIV (train_freq = 543)
Added 19276 words to the vocab in total.
# features: 2
w_embedding.weight: torch.Size([119278, 300])
qemb_match.linear.weight: torch.Size([300, 300])
qemb_match.linear.bias: torch.Size([300])
doc_rnn.rnns.0.weight_ih_l0: torch.Size([1200, 602])
doc_rnn.rnns.0.weight_hh_l0: torch.Size([1200, 300])
doc_rnn.rnns.0.bias_ih_l0: torch.Size([1200])
doc_rnn.rnns.0.bias_hh_l0: torch.Size([1200])
doc_rnn.rnns.0.weight_ih_l0_reverse: torch.Size([1200, 602])
doc_rnn.rnns.0.weight_hh_l0_reverse: torch.Size([1200, 300])
doc_rnn.rnns.0.bias_ih_l0_reverse: torch.Size([1200])
doc_rnn.rnns.0.bias_hh_l0_reverse: torch.Size([1200])
doc_rnn.rnns.1.weight_ih_l0: torch.Size([1200, 600])
doc_rnn.rnns.1.weight_hh_l0: torch.Size([1200, 300])
doc_rnn.rnns.1.bias_ih_l0: torch.Size([1200])
doc_rnn.rnns.1.bias_hh_l0: torch.Size([1200])
doc_rnn.rnns.1.weight_ih_l0_reverse: torch.Size([1200, 600])
doc_rnn.rnns.1.weight_hh_l0_reverse: torch.Size([1200, 300])
doc_rnn.rnns.1.bias_ih_l0_reverse: torch.Size([1200])
doc_rnn.rnns.1.bias_hh_l0_reverse: torch.Size([1200])
doc_rnn.rnns.2.weight_ih_l0: torch.Size([1200, 600])
doc_rnn.rnns.2.weight_hh_l0: torch.Size([1200, 300])
doc_rnn.rnns.2.bias_ih_l0: torch.Size([1200])
doc_rnn.rnns.2.bias_hh_l0: torch.Size([1200])
doc_rnn.rnns.2.weight_ih_l0_reverse: torch.Size([1200, 600])
doc_rnn.rnns.2.weight_hh_l0_reverse: torch.Size([1200, 300])
doc_rnn.rnns.2.bias_ih_l0_reverse: torch.Size([1200])
doc_rnn.rnns.2.bias_hh_l0_reverse: torch.Size([1200])
question_rnn.rnns.0.weight_ih_l0: torch.Size([1200, 300])
question_rnn.rnns.0.weight_hh_l0: torch.Size([1200, 300])
question_rnn.rnns.0.bias_ih_l0: torch.Size([1200])
question_rnn.rnns.0.bias_hh_l0: torch.Size([1200])
question_rnn.rnns.0.weight_ih_l0_reverse: torch.Size([1200, 300])
question_rnn.rnns.0.weight_hh_l0_reverse: torch.Size([1200, 300])
question_rnn.rnns.0.bias_ih_l0_reverse: torch.Size([1200])
question_rnn.rnns.0.bias_hh_l0_reverse: torch.Size([1200])
question_rnn.rnns.1.weight_ih_l0: torch.Size([1200, 600])
question_rnn.rnns.1.weight_hh_l0: torch.Size([1200, 300])
question_rnn.rnns.1.bias_ih_l0: torch.Size([1200])
question_rnn.rnns.1.bias_hh_l0: torch.Size([1200])
question_rnn.rnns.1.weight_ih_l0_reverse: torch.Size([1200, 600])
question_rnn.rnns.1.weight_hh_l0_reverse: torch.Size([1200, 300])
question_rnn.rnns.1.bias_ih_l0_reverse: torch.Size([1200])
question_rnn.rnns.1.bias_hh_l0_reverse: torch.Size([1200])
question_rnn.rnns.2.weight_ih_l0: torch.Size([1200, 600])
question_rnn.rnns.2.weight_hh_l0: torch.Size([1200, 300])
question_rnn.rnns.2.bias_ih_l0: torch.Size([1200])
question_rnn.rnns.2.bias_hh_l0: torch.Size([1200])
question_rnn.rnns.2.weight_ih_l0_reverse: torch.Size([1200, 600])
question_rnn.rnns.2.weight_hh_l0_reverse: torch.Size([1200, 300])
question_rnn.rnns.2.bias_ih_l0_reverse: torch.Size([1200])
question_rnn.rnns.2.bias_hh_l0_reverse: torch.Size([1200])
self_attn.linear.weight: torch.Size([1, 1800])
self_attn.linear.bias: torch.Size([1])
start_attn.linear.weight: torch.Size([1800, 1800])
start_attn.linear.bias: torch.Size([1800])
end_attn.linear.weight: torch.Size([1800, 3600])
end_attn.linear.bias: torch.Size([1800])
#Parameters = 57872701

<> <> <> Starting Timer [Train] <> <> <>

>>> Dev Epoch: [0 / 5]
[predict-0] step: [0 / 665] | f1 = 0.00 | em = 0.00
used_time: 0.21s
[predict-0] step: [400 / 665] | f1 = 0.83 | em = 0.00
used_time: 76.44s
<> <> Timer [Train] <> <> Interval [Validation Epoch 0]: 0h 02m 09s <> <>
Validation Epoch 0 -- F1: 4.10, EM: 0.28 --

>>> Train Epoch: [1 / 5]
[train-1] step: [0 / 9053] | exs = 12 | loss = 11.5521 | f1 = 3.57 | em = 0.00
used_time: 0.60s
[train-1] step: [400 / 9053] | exs = 4812 | loss = 9.7691 | f1 = 1.19 | em = 0.00
used_time: 263.12s
[train-1] step: [800 / 9053] | exs = 9612 | loss = 10.1556 | f1 = 6.55 | em = 0.00
used_time: 537.65s
[train-1] step: [1200 / 9053] | exs = 14412 | loss = 7.7118 | f1 = 36.67 | em = 33.33
used_time: 802.16s
[train-1] step: [1600 / 9053] | exs = 19212 | loss = 9.0882 | f1 = 8.33 | em = 8.33
used_time: 1072.39s
[train-1] step: [2000 / 9053] | exs = 24012 | loss = 6.5456 | f1 = 30.95 | em = 25.00
used_time: 1338.97s
[train-1] step: [2400 / 9053] | exs = 28812 | loss = 7.1615 | f1 = 18.33 | em = 16.67
used_time: 1606.64s
[train-1] step: [2800 / 9053] | exs = 33612 | loss = 8.2514 | f1 = 7.22 | em = 0.00
used_time: 1869.92s
[train-1] step: [3200 / 9053] | exs = 38412 | loss = 7.2324 | f1 = 6.40 | em = 0.00
used_time: 2132.51s
[train-1] step: [3600 / 9053] | exs = 43212 | loss = 9.4908 | f1 = 12.22 | em = 0.00
used_time: 2397.90s
[train-1] step: [4000 / 9053] | exs = 48012 | loss = 9.6022 | f1 = 19.31 | em = 8.33
used_time: 2660.81s
[train-1] step: [4400 / 9053] | exs = 52812 | loss = 7.9738 | f1 = 20.74 | em = 8.33
used_time: 2918.18s
[train-1] step: [4800 / 9053] | exs = 57612 | loss = 7.7257 | f1 = 32.14 | em = 25.00
used_time: 3181.06s
[train-1] step: [5200 / 9053] | exs = 62412 | loss = 7.4443 | f1 = 20.83 | em = 0.00
used_time: 3442.15s
[train-1] step: [5600 / 9053] | exs = 67212 | loss = 7.7634 | f1 = 11.67 | em = 8.33
used_time: 3699.73s
[train-1] step: [6000 / 9053] | exs = 72012 | loss = 7.5359 | f1 = 38.69 | em = 25.00
used_time: 3965.57s
[train-1] step: [6400 / 9053] | exs = 76812 | loss = 8.8156 | f1 = 6.25 | em = 0.00
used_time: 4235.47s
[train-1] step: [6800 / 9053] | exs = 81612 | loss = 8.4374 | f1 = 14.52 | em = 0.00
used_time: 4500.86s
[train-1] step: [7200 / 9053] | exs = 86412 | loss = 7.0593 | f1 = 20.95 | em = 8.33
used_time: 4761.17s
[train-1] step: [7600 / 9053] | exs = 91212 | loss = 8.3464 | f1 = 7.41 | em = 0.00
used_time: 5023.29s
[train-1] step: [8000 / 9053] | exs = 96012 | loss = 7.4352 | f1 = 30.88 | em = 8.33
used_time: 5281.51s
[train-1] step: [8400 / 9053] | exs = 100812 | loss = 6.4074 | f1 = 25.55 | em = 16.67
used_time: 5543.17s
[train-1] step: [8800 / 9053] | exs = 105612 | loss = 6.7197 | f1 = 28.89 | em = 25.00
used_time: 5801.54s
<> <> Timer [Train] <> <> Interval [Training Epoch 1]: 1h 39m 24s <> <>
Training Epoch 1 -- Loss: 7.8791, F1: 19.69, EM: 10.49 --

>>> Dev Epoch: [1 / 5]
[predict-1] step: [0 / 665] | f1 = 36.29 | em = 14.58
used_time: 0.20s
[predict-1] step: [400 / 665] | f1 = 60.07 | em = 50.00
used_time: 75.97s
<> <> Timer [Train] <> <> Interval [Validation Epoch 1]: 0h 02m 07s <> <>
Validation Epoch 1 -- F1: 37.16, EM: 26.13 --
!!! Updated: F1: 37.16, EM: 26.13

>>> Train Epoch: [2 / 5]
[train-2] step: [0 / 9053] | exs = 108659 | loss = 7.6325 | f1 = 10.34 | em = 0.00
used_time: 0.57s
[train-2] step: [400 / 9053] | exs = 113459 | loss = 7.0234 | f1 = 19.05 | em = 16.67
used_time: 262.43s
[train-2] step: [800 / 9053] | exs = 118259 | loss = 6.9928 | f1 = 16.87 | em = 0.00
used_time: 522.28s
[train-2] step: [1200 / 9053] | exs = 123059 | loss = 5.9124 | f1 = 45.96 | em = 41.67
used_time: 783.74s
[train-2] step: [1600 / 9053] | exs = 127859 | loss = 7.2629 | f1 = 21.85 | em = 16.67
used_time: 1040.13s
[train-2] step: [2000 / 9053] | exs = 132659 | loss = 10.7198 | f1 = 17.59 | em = 16.67
used_time: 1295.48s
[train-2] step: [2400 / 9053] | exs = 137459 | loss = 6.1566 | f1 = 26.13 | em = 8.33
used_time: 1555.65s
[train-2] step: [2800 / 9053] | exs = 142259 | loss = 4.8709 | f1 = 40.72 | em = 8.33
used_time: 1815.07s
[train-2] step: [3200 / 9053] | exs = 147059 | loss = 6.2936 | f1 = 37.98 | em = 16.67
used_time: 2081.30s
[train-2] step: [3600 / 9053] | exs = 151859 | loss = 8.5820 | f1 = 17.14 | em = 16.67
used_time: 2340.85s
[train-2] step: [4000 / 9053] | exs = 156659 | loss = 6.2719 | f1 = 29.48 | em = 8.33
used_time: 2597.71s
[train-2] step: [4400 / 9053] | exs = 161459 | loss = 6.4713 | f1 = 34.64 | em = 16.67
used_time: 2860.03s
[train-2] step: [4800 / 9053] | exs = 166259 | loss = 6.9800 | f1 = 22.94 | em = 8.33
used_time: 3122.53s
[train-2] step: [5200 / 9053] | exs = 171059 | loss = 5.4665 | f1 = 27.70 | em = 8.33
used_time: 3387.07s
[train-2] step: [5600 / 9053] | exs = 175859 | loss = 6.2485 | f1 = 14.02 | em = 8.33
used_time: 3665.32s
[train-2] step: [6000 / 9053] | exs = 180659 | loss = 5.1965 | f1 = 19.17 | em = 8.33
used_time: 3932.45s
[train-2] step: [6400 / 9053] | exs = 185459 | loss = 6.7222 | f1 = 22.22 | em = 0.00
used_time: 4209.99s
[train-2] step: [6800 / 9053] | exs = 190259 | loss = 5.2393 | f1 = 31.57 | em = 16.67
used_time: 4473.33s
[train-2] step: [7200 / 9053] | exs = 195059 | loss = 6.3611 | f1 = 30.69 | em = 8.33
used_time: 4746.06s
[train-2] step: [7600 / 9053] | exs = 199859 | loss = 6.5618 | f1 = 27.78 | em = 16.67
used_time: 5012.22s
[train-2] step: [8000 / 9053] | exs = 204659 | loss = 4.3397 | f1 = 55.56 | em = 33.33
used_time: 5284.99s
[train-2] step: [8400 / 9053] | exs = 209459 | loss = 8.0937 | f1 = 38.53 | em = 25.00
used_time: 5548.18s
[train-2] step: [8800 / 9053] | exs = 214259 | loss = 7.3351 | f1 = 19.05 | em = 16.67
used_time: 5827.60s
<> <> Timer [Train] <> <> Interval [Training Epoch 2]: 1h 39m 59s <> <>
Training Epoch 2 -- Loss: 6.6895, F1: 27.89, EM: 16.75 --

>>> Dev Epoch: [2 / 5]
[predict-2] step: [0 / 665] | f1 = 40.90 | em = 14.58
used_time: 0.19s
[predict-2] step: [400 / 665] | f1 = 76.07 | em = 66.67
used_time: 76.74s
<> <> Timer [Train] <> <> Interval [Validation Epoch 2]: 0h 02m 09s <> <>
Validation Epoch 2 -- F1: 42.17, EM: 31.12 --
!!! Updated: F1: 42.17, EM: 31.12

>>> Train Epoch: [3 / 5]
[train-3] step: [0 / 9053] | exs = 217306 | loss = 7.7356 | f1 = 15.74 | em = 8.33
used_time: 0.63s
[train-3] step: [400 / 9053] | exs = 222106 | loss = 7.2830 | f1 = 31.43 | em = 16.67
used_time: 287.35s
[train-3] step: [800 / 9053] | exs = 226906 | loss = 7.4254 | f1 = 29.55 | em = 25.00
used_time: 586.01s
[train-3] step: [1200 / 9053] | exs = 231706 | loss = 6.7630 | f1 = 35.00 | em = 8.33
used_time: 853.68s
[train-3] step: [1600 / 9053] | exs = 236506 | loss = 6.3615 | f1 = 25.27 | em = 16.67
used_time: 1116.02s
[train-3] step: [2000 / 9053] | exs = 241306 | loss = 5.2693 | f1 = 45.42 | em = 33.33
used_time: 1387.24s
[train-3] step: [2400 / 9053] | exs = 246106 | loss = 6.6996 | f1 = 35.32 | em = 25.00
used_time: 1662.43s
[train-3] step: [2800 / 9053] | exs = 250906 | loss = 6.8593 | f1 = 13.06 | em = 0.00
used_time: 1929.18s
[train-3] step: [3200 / 9053] | exs = 255706 | loss = 6.0701 | f1 = 28.76 | em = 16.67
used_time: 2206.99s
[train-3] step: [3600 / 9053] | exs = 260506 | loss = 6.4775 | f1 = 20.83 | em = 16.67
used_time: 2471.70s
[train-3] step: [4000 / 9053] | exs = 265306 | loss = 6.9839 | f1 = 18.06 | em = 8.33
used_time: 2730.61s
[train-3] step: [4400 / 9053] | exs = 270106 | loss = 5.6550 | f1 = 70.19 | em = 58.33
used_time: 2995.28s
[train-3] step: [4800 / 9053] | exs = 274906 | loss = 6.0454 | f1 = 28.24 | em = 16.67
used_time: 3272.51s
[train-3] step: [5200 / 9053] | exs = 279706 | loss = 5.1659 | f1 = 58.33 | em = 33.33
used_time: 3559.87s
[train-3] step: [5600 / 9053] | exs = 284506 | loss = 6.4893 | f1 = 45.88 | em = 33.33
used_time: 3840.24s
[train-3] step: [6000 / 9053] | exs = 289306 | loss = 5.8759 | f1 = 19.10 | em = 8.33
used_time: 4133.71s
[train-3] step: [6400 / 9053] | exs = 294106 | loss = 5.2130 | f1 = 33.96 | em = 16.67
used_time: 4403.22s
[train-3] step: [6800 / 9053] | exs = 298906 | loss = 6.0841 | f1 = 14.96 | em = 8.33
used_time: 4670.36s
[train-3] step: [7200 / 9053] | exs = 303706 | loss = 7.3516 | f1 = 10.10 | em = 0.00
used_time: 4936.12s
[train-3] step: [7600 / 9053] | exs = 308506 | loss = 6.5537 | f1 = 43.59 | em = 33.33
used_time: 5202.52s
[train-3] step: [8000 / 9053] | exs = 313306 | loss = 6.2572 | f1 = 48.40 | em = 41.67
used_time: 5473.48s
[train-3] step: [8400 / 9053] | exs = 318106 | loss = 4.6305 | f1 = 46.67 | em = 25.00
used_time: 5743.21s
[train-3] step: [8800 / 9053] | exs = 322906 | loss = 5.5512 | f1 = 27.30 | em = 16.67
used_time: 6015.42s
<> <> Timer [Train] <> <> Interval [Training Epoch 3]: 1h 43m 04s <> <>
Training Epoch 3 -- Loss: 6.2339, F1: 31.30, EM: 19.64 --

>>> Dev Epoch: [3 / 5]
[predict-3] step: [0 / 665] | f1 = 54.26 | em = 31.25
used_time: 0.20s
[predict-3] step: [400 / 665] | f1 = 73.96 | em = 66.67
used_time: 78.12s
<> <> Timer [Train] <> <> Interval [Validation Epoch 3]: 0h 02m 10s <> <>
Validation Epoch 3 -- F1: 44.90, EM: 33.65 --
!!! Updated: F1: 44.90, EM: 33.65

>>> Train Epoch: [4 / 5]
[train-4] step: [0 / 9053] | exs = 325953 | loss = 6.1480 | f1 = 38.04 | em = 8.33
used_time: 0.58s
[train-4] step: [400 / 9053] | exs = 330753 | loss = 3.9205 | f1 = 58.50 | em = 41.67
used_time: 269.16s
[train-4] step: [800 / 9053] | exs = 335553 | loss = 7.3778 | f1 = 18.75 | em = 0.00
used_time: 543.46s
[train-4] step: [1200 / 9053] | exs = 340353 | loss = 6.2266 | f1 = 38.89 | em = 33.33
used_time: 811.59s
[train-4] step: [1600 / 9053] | exs = 345153 | loss = 5.3186 | f1 = 42.51 | em = 25.00
used_time: 1083.31s
[train-4] step: [2000 / 9053] | exs = 349953 | loss = 5.0935 | f1 = 24.62 | em = 8.33
used_time: 1344.99s
[train-4] step: [2400 / 9053] | exs = 354753 | loss = 6.4481 | f1 = 45.48 | em = 33.33
used_time: 1601.43s
[train-4] step: [2800 / 9053] | exs = 359553 | loss = 5.9833 | f1 = 19.34 | em = 8.33
used_time: 1859.65s
[train-4] step: [3200 / 9053] | exs = 364353 | loss = 5.3428 | f1 = 47.98 | em = 33.33
used_time: 2120.46s
[train-4] step: [3600 / 9053] | exs = 369153 | loss = 8.4403 | f1 = 30.13 | em = 25.00
used_time: 2379.96s
[train-4] step: [4000 / 9053] | exs = 373953 | loss = 5.5548 | f1 = 32.44 | em = 25.00
used_time: 2645.85s
[train-4] step: [4400 / 9053] | exs = 378753 | loss = 6.3381 | f1 = 22.94 | em = 8.33
used_time: 2915.48s
[train-4] step: [4800 / 9053] | exs = 383553 | loss = 6.2645 | f1 = 33.33 | em = 33.33
used_time: 3184.04s
[train-4] step: [5200 / 9053] | exs = 388353 | loss = 5.9403 | f1 = 30.16 | em = 16.67
used_time: 3451.20s
[train-4] step: [5600 / 9053] | exs = 393153 | loss = 6.8257 | f1 = 24.35 | em = 8.33
used_time: 3711.90s
[train-4] step: [6000 / 9053] | exs = 397953 | loss = 5.2677 | f1 = 54.35 | em = 33.33
used_time: 4019.89s
[train-4] step: [6400 / 9053] | exs = 402753 | loss = 4.4343 | f1 = 42.88 | em = 25.00
used_time: 4285.67s
[train-4] step: [6800 / 9053] | exs = 407553 | loss = 5.4106 | f1 = 28.24 | em = 16.67
used_time: 4549.48s
[train-4] step: [7200 / 9053] | exs = 412353 | loss = 7.9289 | f1 = 28.12 | em = 8.33
used_time: 4814.84s
[train-4] step: [7600 / 9053] | exs = 417153 | loss = 4.9884 | f1 = 44.77 | em = 16.67
used_time: 5082.73s
[train-4] step: [8000 / 9053] | exs = 421953 | loss = 4.1309 | f1 = 51.86 | em = 33.33
used_time: 5349.74s
[train-4] step: [8400 / 9053] | exs = 426753 | loss = 7.4298 | f1 = 26.28 | em = 25.00
used_time: 5613.76s
[train-4] step: [8800 / 9053] | exs = 431553 | loss = 6.7505 | f1 = 36.67 | em = 25.00
used_time: 5878.09s
<> <> Timer [Train] <> <> Interval [Training Epoch 4]: 1h 40m 45s <> <>
Training Epoch 4 -- Loss: 5.9259, F1: 33.65, EM: 21.59 --

>>> Dev Epoch: [4 / 5]
[predict-4] step: [0 / 665] | f1 = 36.09 | em = 22.92
used_time: 0.21s
[predict-4] step: [400 / 665] | f1 = 76.23 | em = 66.67
used_time: 81.55s
<> <> Timer [Train] <> <> Interval [Validation Epoch 4]: 0h 02m 18s <> <>
Validation Epoch 4 -- F1: 46.24, EM: 34.68 --
!!! Updated: F1: 46.24, EM: 34.68

>>> Train Epoch: [5 / 5]
[train-5] step: [0 / 9053] | exs = 434600 | loss = 6.4891 | f1 = 33.57 | em = 16.67
used_time: 0.67s
[train-5] step: [400 / 9053] | exs = 439400 | loss = 3.6246 | f1 = 44.44 | em = 41.67
used_time: 261.21s
[train-5] step: [800 / 9053] | exs = 444200 | loss = 6.2954 | f1 = 22.22 | em = 16.67
used_time: 528.31s
[train-5] step: [1200 / 9053] | exs = 449000 | loss = 4.4652 | f1 = 45.13 | em = 33.33
used_time: 813.60s
[train-5] step: [1600 / 9053] | exs = 453800 | loss = 5.2841 | f1 = 25.95 | em = 16.67
used_time: 1084.52s
[train-5] step: [2000 / 9053] | exs = 458600 | loss = 4.4641 | f1 = 51.24 | em = 25.00
used_time: 1347.42s
[train-5] step: [2400 / 9053] | exs = 463400 | loss = 8.1783 | f1 = 26.48 | em = 8.33
used_time: 1613.89s
[train-5] step: [2800 / 9053] | exs = 468200 | loss = 3.8517 | f1 = 26.39 | em = 16.67
used_time: 1876.29s
[train-5] step: [3200 / 9053] | exs = 473000 | loss = 5.0473 | f1 = 48.73 | em = 25.00
used_time: 2139.31s
[train-5] step: [3600 / 9053] | exs = 477800 | loss = 5.4944 | f1 = 46.08 | em = 33.33
used_time: 2399.89s
[train-5] step: [4000 / 9053] | exs = 482600 | loss = 6.3403 | f1 = 27.60 | em = 16.67
used_time: 2662.00s
[train-5] step: [4400 / 9053] | exs = 487400 | loss = 5.7836 | f1 = 35.20 | em = 16.67
used_time: 2929.55s
[train-5] step: [4800 / 9053] | exs = 492200 | loss = 6.0548 | f1 = 45.83 | em = 41.67
used_time: 3190.85s
[train-5] step: [5200 / 9053] | exs = 497000 | loss = 7.1048 | f1 = 16.67 | em = 16.67
used_time: 3448.01s
[train-5] step: [5600 / 9053] | exs = 501800 | loss = 6.3933 | f1 = 29.76 | em = 25.00
used_time: 3707.47s
[train-5] step: [6000 / 9053] | exs = 506600 | loss = 4.5054 | f1 = 42.13 | em = 25.00
used_time: 3963.12s
[train-5] step: [6400 / 9053] | exs = 511400 | loss = 2.2041 | f1 = 73.89 | em = 58.33
used_time: 4221.00s
[train-5] step: [6800 / 9053] | exs = 516200 | loss = 6.3080 | f1 = 21.72 | em = 16.67
used_time: 4486.38s
[train-5] step: [7200 / 9053] | exs = 521000 | loss = 7.4128 | f1 = 29.00 | em = 8.33
used_time: 4751.63s
[train-5] step: [7600 / 9053] | exs = 525800 | loss = 7.1956 | f1 = 17.38 | em = 8.33
used_time: 5017.95s
[train-5] step: [8000 / 9053] | exs = 530600 | loss = 5.9885 | f1 = 13.12 | em = 8.33
used_time: 5282.48s
[train-5] step: [8400 / 9053] | exs = 535400 | loss = 6.2210 | f1 = 5.75 | em = 0.00
used_time: 5549.58s
[train-5] step: [8800 / 9053] | exs = 540200 | loss = 7.1285 | f1 = 7.92 | em = 0.00
used_time: 5820.13s
<> <> Timer [Train] <> <> Interval [Training Epoch 5]: 1h 39m 47s <> <>
Training Epoch 5 -- Loss: 5.6998, F1: 35.07, EM: 22.85 --

>>> Dev Epoch: [5 / 5]
[predict-5] step: [0 / 665] | f1 = 60.63 | em = 39.58
used_time: 0.20s
[predict-5] step: [400 / 665] | f1 = 75.00 | em = 66.67
used_time: 78.10s
<> <> Timer [Train] <> <> Interval [Validation Epoch 5]: 0h 02m 12s <> <>
Validation Epoch 5 -- F1: 47.35, EM: 36.05 --
!!! Updated: F1: 47.35, EM: 36.05
<> <> <> Finished Timer [Train] <> <> <> Total time elapsed: 8h 36m 11s <> <> <>
Finished Training: pipeline_models
 <<<<<<<<<<<<<<<< MODEL SUMMARY >>>>>>>>>>>>>>>> 
Best epoch = 5
Dev F1 = 47.35
Dev EM = 36.05
 <<<<<<<<<<<<<<<< MODEL SUMMARY >>>>>>>>>>>>>>>> 
No testing set specified -- skipped testing.
